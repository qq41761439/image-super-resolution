{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Image Super-Resolution (ISR) The goal of this project is to upscale and improve the quality of low resolution images. This project contains Keras implementations of different Residual Dense Networks for Single Image Super-Resolution (ISR) as well as scripts to train these networks using content and adversarial loss components. The implemented networks include: The super-scaling Residual Dense Network described in Residual Dense Network for Image Super-Resolution (Zhang et al. 2018) The super-scaling Residual in Residual Dense Network described in ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks (Wang et al. 2018) A multi-output version of the Keras VGG19 network for deep features extraction used in the perceptual loss A custom discriminator network based on the one described in Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (SRGANS, Ledig et al. 2017) Read the full documentation at: https://idealo.github.io/image-super-resolution/ . Docker scripts and Google Colab notebooks are available to carry training and prediction. Also, we provide scripts to facilitate training on the cloud with AWS and nvidia-docker with only a few commands. ISR is compatible with Python 3.6 and is distributed under the Apache 2.0 license. We welcome any kind of contribution. If you wish to contribute, please see the Contribute section. Contents Sample Results Installation Usage Additional Information Contribute Citation Maintainers License Sample Results The samples are upscaled with a factor of two. The weights used to produced these images are available under sample_weights (see Additional Information ). They are stored on git lfs . If you want to download the weights you need to run git lfs pull after cloning the repository. The original low resolution image (left), the super scaled output of the network (center) and the result of the baseline scaling obtained with GIMP bicubic scaling (right). Below a comparison of different methods on a noisy image: the baseline, bicubic scaling; the RDN network trained using a pixel-wise content loss (PSNR-driven); the same network re-trained on a compressed dataset using VGG19-content and adversarial components for the loss (VGG+GANs). The weights used here are available in this repo. Bicubic up-scaling (baseline). RDN trained with pixel-wise content loss (PSNR-driven). RDN trained with a VGG content and adversarial loss components.. Installation There are two ways to install the Image Super-Resolution package: Install ISR from PyPI (recommended): pip install ISR Install ISR from the GitHub source: git clone https : // github . com / idealo / image - super - resolution cd image - super - resolution git lfs pull python setup . py install Usage Prediction Load image and prepare it import numpy as np from PIL import Image img = Image . open ( 'data/input/test_images/sample_image.jpg' ) lr_img = np . array ( img ) Load model and run prediction from ISR.models import RDN rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/sample_weights/rdn-C6-D20-G64-G064-x2/ArtefactCancelling/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5' ) sr_img = rdn . predict ( lr_img ) Image . fromarray ( sr_img ) Training Create the models from ISR.models import RRDN from ISR.models import Discriminator from ISR.models import Cut_VGG19 lr_train_patch_size = 40 layers_to_extract = [ 5 , 9 ] scale = 2 hr_train_patch_size = lr_train_patch_size * scale rrdn = RRDN ( arch_params = { 'C' : 4 , 'D' : 3 , 'G' : 64 , 'G0' : 64 , 'T' : 10 , 'x' : scale }, patch_size = lr_train_patch_size ) f_ext = Cut_VGG19 ( patch_size = hr_train_patch_size , layers_to_extract = layers_to_extract ) discr = Discriminator ( patch_size = hr_train_patch_size , kernel_size = 3 ) Create a Trainer object using the desired settings and give it the models ( f_ext and discr are optional) from ISR.train import Trainer loss_weights = { 'generator' : 0.0 , 'feature_extractor' : 0.0833 , 'discriminator' : 0.01 , } trainer = Trainer ( generator = rrdn , discriminator = discr , feature_extractor = f_ext , lr_train_dir = 'low_res/training/images' , hr_train_dir = 'high_res/training/images' , lr_valid_dir = 'low_res/validation/images' , hr_valid_dir = 'high_res/validation/images' , loss_weights = loss_weights , dataname = 'image_dataset' , logs_dir = './logs' , weights_dir = './weights' , weights_generator = None , weights_discriminator = None , n_validation = 40 , lr_decay_frequency = 30 , lr_decay_factor = 0.5 , ) Start training trainer . train ( epochs = 80 , steps_per_epoch = 500 , batch_size = 16 , ) Additional Information You can read about how we trained these network weights in our Medium posts: - part 1: A deep learning based magnifying glass - part 2: Zoom in... enhance RDN Pre-trained weights The weights of the RDN network trained on the DIV2K dataset are available in weights/sample_weights/rdn-C6-D20-G64-G064-x2/PSNR-driven/rdn-C6-D20-G64-G064-x2_PSNR_epoch086.hdf5 . The model was trained using C=6, D=20, G=64, G0=64 as parameters (see architecture for details) for 86 epochs of 1000 batches of 8 32x32 augmented patches taken from LR images. The artefact can cancelling weights obtained with a combination of different training sessions using different datasets and perceptual loss with VGG19 and GAN can be found at weights/sample_weights/rdn-C6-D20-G64-G064-x2/ArtefactCancelling/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5 We recommend using these weights only when cancelling compression artefacts is a desirable effect. RDN Network architecture The main parameters of the architecture structure are: - D - number of Residual Dense Blocks (RDB) - C - number of convolutional layers stacked inside a RDB - G - number of feature maps of each convolutional layers inside the RDBs - G0 - number of feature maps for convolutions outside of RDBs and of each RBD output source: Residual Dense Network for Image Super-Resolution RRDN Network architecture The main parameters of the architecture structure are: - T - number of Residual in Residual Dense Blocks (RRDB) - D - number of Residual Dense Blocks (RDB) insider each RRDB - C - number of convolutional layers stacked inside a RDB - G - number of feature maps of each convolutional layers inside the RDBs - G0 - number of feature maps for convolutions outside of RDBs and of each RBD output source: ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks Contribute We welcome all kinds of contributions, models trained on different datasets, new model architectures and/or hyperparameters combinations that improve the performance of the currently published model. Will publish the performances of new models in this repository. See the Contribution guide for more details. Citation Please cite our work in your publications if it helps your research. @misc { cardinale2018isr , title = { ISR } , author = { Francesco Cardinale et al . } , year = { 2018 } , howpublished = {\\ url { https : // github . com / idealo / image - super - resolution }} , } Maintainers Francesco Cardinale, github: cfrancesco Zubin John, github: valiantone Dat Tran, github: datitran Copyright See LICENSE for details.","title":"Home"},{"location":"#image-super-resolution-isr","text":"The goal of this project is to upscale and improve the quality of low resolution images. This project contains Keras implementations of different Residual Dense Networks for Single Image Super-Resolution (ISR) as well as scripts to train these networks using content and adversarial loss components. The implemented networks include: The super-scaling Residual Dense Network described in Residual Dense Network for Image Super-Resolution (Zhang et al. 2018) The super-scaling Residual in Residual Dense Network described in ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks (Wang et al. 2018) A multi-output version of the Keras VGG19 network for deep features extraction used in the perceptual loss A custom discriminator network based on the one described in Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (SRGANS, Ledig et al. 2017) Read the full documentation at: https://idealo.github.io/image-super-resolution/ . Docker scripts and Google Colab notebooks are available to carry training and prediction. Also, we provide scripts to facilitate training on the cloud with AWS and nvidia-docker with only a few commands. ISR is compatible with Python 3.6 and is distributed under the Apache 2.0 license. We welcome any kind of contribution. If you wish to contribute, please see the Contribute section.","title":"Image Super-Resolution (ISR)"},{"location":"#contents","text":"Sample Results Installation Usage Additional Information Contribute Citation Maintainers License","title":"Contents"},{"location":"#sample-results","text":"The samples are upscaled with a factor of two. The weights used to produced these images are available under sample_weights (see Additional Information ). They are stored on git lfs . If you want to download the weights you need to run git lfs pull after cloning the repository. The original low resolution image (left), the super scaled output of the network (center) and the result of the baseline scaling obtained with GIMP bicubic scaling (right). Below a comparison of different methods on a noisy image: the baseline, bicubic scaling; the RDN network trained using a pixel-wise content loss (PSNR-driven); the same network re-trained on a compressed dataset using VGG19-content and adversarial components for the loss (VGG+GANs). The weights used here are available in this repo. Bicubic up-scaling (baseline). RDN trained with pixel-wise content loss (PSNR-driven). RDN trained with a VGG content and adversarial loss components..","title":"Sample Results"},{"location":"#installation","text":"There are two ways to install the Image Super-Resolution package: Install ISR from PyPI (recommended): pip install ISR Install ISR from the GitHub source: git clone https : // github . com / idealo / image - super - resolution cd image - super - resolution git lfs pull python setup . py install","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#prediction","text":"Load image and prepare it import numpy as np from PIL import Image img = Image . open ( 'data/input/test_images/sample_image.jpg' ) lr_img = np . array ( img ) Load model and run prediction from ISR.models import RDN rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/sample_weights/rdn-C6-D20-G64-G064-x2/ArtefactCancelling/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5' ) sr_img = rdn . predict ( lr_img ) Image . fromarray ( sr_img )","title":"Prediction"},{"location":"#training","text":"Create the models from ISR.models import RRDN from ISR.models import Discriminator from ISR.models import Cut_VGG19 lr_train_patch_size = 40 layers_to_extract = [ 5 , 9 ] scale = 2 hr_train_patch_size = lr_train_patch_size * scale rrdn = RRDN ( arch_params = { 'C' : 4 , 'D' : 3 , 'G' : 64 , 'G0' : 64 , 'T' : 10 , 'x' : scale }, patch_size = lr_train_patch_size ) f_ext = Cut_VGG19 ( patch_size = hr_train_patch_size , layers_to_extract = layers_to_extract ) discr = Discriminator ( patch_size = hr_train_patch_size , kernel_size = 3 ) Create a Trainer object using the desired settings and give it the models ( f_ext and discr are optional) from ISR.train import Trainer loss_weights = { 'generator' : 0.0 , 'feature_extractor' : 0.0833 , 'discriminator' : 0.01 , } trainer = Trainer ( generator = rrdn , discriminator = discr , feature_extractor = f_ext , lr_train_dir = 'low_res/training/images' , hr_train_dir = 'high_res/training/images' , lr_valid_dir = 'low_res/validation/images' , hr_valid_dir = 'high_res/validation/images' , loss_weights = loss_weights , dataname = 'image_dataset' , logs_dir = './logs' , weights_dir = './weights' , weights_generator = None , weights_discriminator = None , n_validation = 40 , lr_decay_frequency = 30 , lr_decay_factor = 0.5 , ) Start training trainer . train ( epochs = 80 , steps_per_epoch = 500 , batch_size = 16 , )","title":"Training"},{"location":"#additional-information","text":"You can read about how we trained these network weights in our Medium posts: - part 1: A deep learning based magnifying glass - part 2: Zoom in... enhance","title":"Additional Information"},{"location":"#rdn-pre-trained-weights","text":"The weights of the RDN network trained on the DIV2K dataset are available in weights/sample_weights/rdn-C6-D20-G64-G064-x2/PSNR-driven/rdn-C6-D20-G64-G064-x2_PSNR_epoch086.hdf5 . The model was trained using C=6, D=20, G=64, G0=64 as parameters (see architecture for details) for 86 epochs of 1000 batches of 8 32x32 augmented patches taken from LR images. The artefact can cancelling weights obtained with a combination of different training sessions using different datasets and perceptual loss with VGG19 and GAN can be found at weights/sample_weights/rdn-C6-D20-G64-G064-x2/ArtefactCancelling/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5 We recommend using these weights only when cancelling compression artefacts is a desirable effect.","title":"RDN Pre-trained weights"},{"location":"#rdn-network-architecture","text":"The main parameters of the architecture structure are: - D - number of Residual Dense Blocks (RDB) - C - number of convolutional layers stacked inside a RDB - G - number of feature maps of each convolutional layers inside the RDBs - G0 - number of feature maps for convolutions outside of RDBs and of each RBD output source: Residual Dense Network for Image Super-Resolution","title":"RDN Network architecture"},{"location":"#rrdn-network-architecture","text":"The main parameters of the architecture structure are: - T - number of Residual in Residual Dense Blocks (RRDB) - D - number of Residual Dense Blocks (RDB) insider each RRDB - C - number of convolutional layers stacked inside a RDB - G - number of feature maps of each convolutional layers inside the RDBs - G0 - number of feature maps for convolutions outside of RDBs and of each RBD output source: ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks","title":"RRDN Network architecture"},{"location":"#contribute","text":"We welcome all kinds of contributions, models trained on different datasets, new model architectures and/or hyperparameters combinations that improve the performance of the currently published model. Will publish the performances of new models in this repository. See the Contribution guide for more details.","title":"Contribute"},{"location":"#citation","text":"Please cite our work in your publications if it helps your research. @misc { cardinale2018isr , title = { ISR } , author = { Francesco Cardinale et al . } , year = { 2018 } , howpublished = {\\ url { https : // github . com / idealo / image - super - resolution }} , }","title":"Citation"},{"location":"#maintainers","text":"Francesco Cardinale, github: cfrancesco Zubin John, github: valiantone Dat Tran, github: datitran","title":"Maintainers"},{"location":"#copyright","text":"See LICENSE for details.","title":"Copyright"},{"location":"CONTRIBUTING/","text":"Contribution Guide We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Image Super-Resolution repo. Documentation Make sure any new function or class you introduce has proper docstrings. Testing We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contribution"},{"location":"CONTRIBUTING/#contribution-guide","text":"We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING/#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs:"},{"location":"CONTRIBUTING/#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING/#pull-requests-pr","text":"Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Image Super-Resolution repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING/#documentation","text":"Make sure any new function or class you introduce has proper docstrings.","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING/#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE/","text":"Copyright 2018 idealo internet GmbH. All rights reserved. Apache License Version 2 . 0 , January 2004 http : // www . apache . org / licenses / TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work , attach the following boilerplate notice , with the fields enclosed by brackets \" [] \" replaced with your own identifying information . ( Don ' t include the brackets ! ) The text should be enclosed in the appropriate comment syntax for the file format . We also recommend that a file or class name and description of purpose be included on the same \" printed page \" as the copyright notice for easier identification within third - party archives . Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http : // www . apache . org / licenses / LICENSE - 2 . 0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"assistant/","text":"run def run ( config_file , default , training , prediction )","title":"Assistant"},{"location":"assistant/#run","text":"def run ( config_file , default , training , prediction )","title":"run"},{"location":"models/cut_vgg19/","text":"class Cut_VGG19 Class object that fetches keras' VGG19 model trained on the imagenet dataset and declares as output layers. Used as feature extractor for the perceptual loss function. Args layers_to_extract : list of layers to be declared as output layers. patch_size : integer, defines the size of the input (patch_size x patch_size). Attributes loss_model : multi-output vgg architecture with as output layers. __init__ def __init__ ( patch_size , layers_to_extract )","title":"Cut VGG19"},{"location":"models/cut_vgg19/#class-cut_vgg19","text":"Class object that fetches keras' VGG19 model trained on the imagenet dataset and declares as output layers. Used as feature extractor for the perceptual loss function.","title":"class Cut_VGG19"},{"location":"models/cut_vgg19/#args","text":"layers_to_extract : list of layers to be declared as output layers. patch_size : integer, defines the size of the input (patch_size x patch_size).","title":"Args"},{"location":"models/cut_vgg19/#attributes","text":"loss_model : multi-output vgg architecture with as output layers.","title":"Attributes"},{"location":"models/cut_vgg19/#9595init9595","text":"def __init__ ( patch_size , layers_to_extract )","title":"__init__"},{"location":"models/discriminator/","text":"class Discriminator Implementation of the discriminator network for the adversarial component of the perceptual loss. Args patch_size : integer, determines input size as (patch_size, patch_size, 3). kernel_size : size of the kernel in the conv blocks. Attributes model : Keras model. name : name used to identify what discriminator is used during GANs training. model.name : identifies this network as the discriminator network in the compound model built by the trainer class. block_param : dictionary, determines the number of filters and the strides for each conv block. __init__ def __init__ ( patch_size , kernel_size )","title":"Discriminator"},{"location":"models/discriminator/#class-discriminator","text":"Implementation of the discriminator network for the adversarial component of the perceptual loss.","title":"class Discriminator"},{"location":"models/discriminator/#args","text":"patch_size : integer, determines input size as (patch_size, patch_size, 3). kernel_size : size of the kernel in the conv blocks.","title":"Args"},{"location":"models/discriminator/#attributes","text":"model : Keras model. name : name used to identify what discriminator is used during GANs training. model.name : identifies this network as the discriminator network in the compound model built by the trainer class. block_param : dictionary, determines the number of filters and the strides for each conv block.","title":"Attributes"},{"location":"models/discriminator/#9595init9595","text":"def __init__ ( patch_size , kernel_size )","title":"__init__"},{"location":"models/imagemodel/","text":"class ImageModel ISR models parent class. Contains functions that are common across the super-scaling models. predict def predict ( input_image_array ) Processes the image array into a suitable format and transforms the network output in a suitable image format. Args input_image_array : input image array. Returns sr_img : image output.","title":"Imagemodel"},{"location":"models/imagemodel/#class-imagemodel","text":"ISR models parent class. Contains functions that are common across the super-scaling models.","title":"class ImageModel"},{"location":"models/imagemodel/#predict","text":"def predict ( input_image_array ) Processes the image array into a suitable format and transforms the network output in a suitable image format.","title":"predict"},{"location":"models/imagemodel/#args","text":"input_image_array : input image array.","title":"Args"},{"location":"models/imagemodel/#returns","text":"sr_img : image output.","title":"Returns"},{"location":"models/rdn/","text":"make_model def make_model ( arch_params , patch_size ) Returns the model. Used to select the model. class RDN Implementation of the Residual Dense Network for image super-scaling. The network is the one described in https://arxiv.org/abs/1802.08797 (Zhang et al. 2018). Args arch_params : dictionary, contains the network parameters C, D, G, G0, x. patch_size : integer or None, determines the input size. Only needed at training time, for prediction is set to None. c_dim : integer, number of channels of the input image. kernel_size : integer, common kernel size for convolutions. upscaling : string, 'ups' or 'shuffle', determines which implementation of the upscaling layer to use. init_extreme_val : extreme values for the RandomUniform initializer. Attributes C : integer, number of conv layer inside each residual dense blocks (RDB). D : integer, number of RDBs. G : integer, number of convolution output filters inside the RDBs. G0 : integer, number of output filters of each RDB. x : integer, the scaling factor. model : Keras model of the RDN. name : name used to identify what upscaling network is used during training. model.name : identifies this network as the generator network in the compound model built by the trainer class. __init__ def __init__ ( arch_params , patch_size , c_dim , kernel_size , upscaling , init_extreme_val )","title":"RDN"},{"location":"models/rdn/#make95model","text":"def make_model ( arch_params , patch_size ) Returns the model. Used to select the model.","title":"make_model"},{"location":"models/rdn/#class-rdn","text":"Implementation of the Residual Dense Network for image super-scaling. The network is the one described in https://arxiv.org/abs/1802.08797 (Zhang et al. 2018).","title":"class RDN"},{"location":"models/rdn/#args","text":"arch_params : dictionary, contains the network parameters C, D, G, G0, x. patch_size : integer or None, determines the input size. Only needed at training time, for prediction is set to None. c_dim : integer, number of channels of the input image. kernel_size : integer, common kernel size for convolutions. upscaling : string, 'ups' or 'shuffle', determines which implementation of the upscaling layer to use. init_extreme_val : extreme values for the RandomUniform initializer.","title":"Args"},{"location":"models/rdn/#attributes","text":"C : integer, number of conv layer inside each residual dense blocks (RDB). D : integer, number of RDBs. G : integer, number of convolution output filters inside the RDBs. G0 : integer, number of output filters of each RDB. x : integer, the scaling factor. model : Keras model of the RDN. name : name used to identify what upscaling network is used during training. model.name : identifies this network as the generator network in the compound model built by the trainer class.","title":"Attributes"},{"location":"models/rdn/#9595init9595","text":"def __init__ ( arch_params , patch_size , c_dim , kernel_size , upscaling , init_extreme_val )","title":"__init__"},{"location":"models/rrdn/","text":"make_model def make_model ( arch_params , patch_size ) Returns the model. Used to select the model. class RRDN Implementation of the Residual in Residual Dense Network for image super-scaling. The network is the one described in https://arxiv.org/abs/1809.00219 (Wang et al. 2018). Args arch_params : dictionary, contains the network parameters C, D, G, G0, T, x. patch_size : integer or None, determines the input size. Only needed at training time, for prediction is set to None. beta : float <= 1, scaling parameter for the residual connections. c_dim : integer, number of channels of the input image. kernel_size : integer, common kernel size for convolutions. upscaling : string, 'ups' or 'shuffle', determines which implementation of the upscaling layer to use. init_val : extreme values for the RandomUniform initializer. Attributes C : integer, number of conv layer inside each residual dense blocks (RDB). D : integer, number of RDBs inside each Residual in Residual Dense Block (RRDB). T : integer, number or RRDBs. G : integer, number of convolution output filters inside the RDBs. G0 : integer, number of output filters of each RDB. x : integer, the scaling factor. model : Keras model of the RRDN. name : name used to identify what upscaling network is used during training. model.name : identifies this network as the generator network in the compound model built by the trainer class. __init__ def __init__ ( arch_params , patch_size , beta , c_dim , kernel_size , init_val )","title":"RRDN"},{"location":"models/rrdn/#make95model","text":"def make_model ( arch_params , patch_size ) Returns the model. Used to select the model.","title":"make_model"},{"location":"models/rrdn/#class-rrdn","text":"Implementation of the Residual in Residual Dense Network for image super-scaling. The network is the one described in https://arxiv.org/abs/1809.00219 (Wang et al. 2018).","title":"class RRDN"},{"location":"models/rrdn/#args","text":"arch_params : dictionary, contains the network parameters C, D, G, G0, T, x. patch_size : integer or None, determines the input size. Only needed at training time, for prediction is set to None. beta : float <= 1, scaling parameter for the residual connections. c_dim : integer, number of channels of the input image. kernel_size : integer, common kernel size for convolutions. upscaling : string, 'ups' or 'shuffle', determines which implementation of the upscaling layer to use. init_val : extreme values for the RandomUniform initializer.","title":"Args"},{"location":"models/rrdn/#attributes","text":"C : integer, number of conv layer inside each residual dense blocks (RDB). D : integer, number of RDBs inside each Residual in Residual Dense Block (RRDB). T : integer, number or RRDBs. G : integer, number of convolution output filters inside the RDBs. G0 : integer, number of output filters of each RDB. x : integer, the scaling factor. model : Keras model of the RRDN. name : name used to identify what upscaling network is used during training. model.name : identifies this network as the generator network in the compound model built by the trainer class.","title":"Attributes"},{"location":"models/rrdn/#9595init9595","text":"def __init__ ( arch_params , patch_size , beta , c_dim , kernel_size , init_val )","title":"__init__"},{"location":"predict/predictor/","text":"class Predictor The predictor class handles prediction, given an input model. Loads the images in the input directory, executes training given a model and saves the results in the output directory. Can receive a path for the weights or can let the user browse through the weights directory for the desired weights. Args input_dir : string, path to the input directory. output_dir : string, path to the output directory. verbose : bool. Attributes extensions : list of accepted image extensions. img_ls : list of image files in input_dir. Methods get_predictions : given a model and a string containing the weights' path, runs the predictions on the images contained in the input directory and stores the results in the output directory. __init__ def __init__ ( input_dir , output_dir , verbose ) get_predictions def get_predictions ( model , weights_path ) Runs the prediction.","title":"Predict"},{"location":"predict/predictor/#class-predictor","text":"The predictor class handles prediction, given an input model. Loads the images in the input directory, executes training given a model and saves the results in the output directory. Can receive a path for the weights or can let the user browse through the weights directory for the desired weights.","title":"class Predictor"},{"location":"predict/predictor/#args","text":"input_dir : string, path to the input directory. output_dir : string, path to the output directory. verbose : bool.","title":"Args"},{"location":"predict/predictor/#attributes","text":"extensions : list of accepted image extensions. img_ls : list of image files in input_dir.","title":"Attributes"},{"location":"predict/predictor/#methods","text":"get_predictions : given a model and a string containing the weights' path, runs the predictions on the images contained in the input directory and stores the results in the output directory.","title":"Methods"},{"location":"predict/predictor/#9595init9595","text":"def __init__ ( input_dir , output_dir , verbose )","title":"__init__"},{"location":"predict/predictor/#get95predictions","text":"def get_predictions ( model , weights_path ) Runs the prediction.","title":"get_predictions"},{"location":"train/trainer/","text":"class Trainer Class object to setup and carry the training. Takes as input a generator that produces SR images. Conditionally, also a discriminator network and a feature extractor to build the components of the perceptual loss. Compiles the model(s) and trains in a GANS fashion if a discriminator is provided, otherwise carries a regular ISR training. Args generator : Keras model, the super-scaling, or generator, network. discriminator : Keras model, the discriminator network for the adversarial component of the perceptual loss. feature_extractor : Keras model, feature extractor network for the deep features component of perceptual loss function. lr_train_dir : path to the directory containing the Low-Res images for training. hr_train_dir : path to the directory containing the High-Res images for training. lr_valid_dir : path to the directory containing the Low-Res images for validation. hr_valid_dir : path to the directory containing the High-Res images for validation. learning_rate : float. loss_weights : dictionary, use to weigh the components of the loss function. Contains 'generator' for the generator loss component, and can contain 'discriminator' and 'feature_extractor' for the discriminator and deep features components respectively. logs_dir : path to the directory where the tensorboard logs are saved. weights_dir : path to the directory where the weights are saved. dataname : string, used to identify what dataset is used for the training session. weights_generator : path to the pre-trained generator's weights, for transfer learning. weights_discriminator : path to the pre-trained discriminator's weights, for transfer learning. n_validation : integer, number of validation samples used at training from the validation set. flatness : dictionary. Determines determines the 'flatness' threshold level for the training patches. See the TrainerHelper class for more details. lr_decay_frequency : integer, every how many epochs the learning rate is reduced. lr_decay_factor : 0 < float <1, learning rate reduction multiplicative factor. Methods train : combines the networks and triggers training with the specified settings. __init__ def __init__ ( generator , discriminator , feature_extractor , lr_train_dir , hr_train_dir , lr_valid_dir , hr_valid_dir , loss_weights , log_dirs , fallback_save_every_n_epochs , dataname , weights_generator , weights_discriminator , n_validation , flatness , learning_rate , adam_optimizer , losses , metrics ) update_training_config def update_training_config ( settings ) Summarizes training setting. train def train ( epochs , steps_per_epoch , batch_size , monitored_metrics ) Carries on the training for the given number of epochs. Sends the losses to Tensorboard. Args epochs : how many epochs to train for. steps_per_epoch : how many batches epoch. batch_size : amount of images per batch. monitored_metrics : dictionary, the keys are the metrics that are monitored for the weights saving logic. The values are the mode that trigger the weights saving ('min' vs 'max').","title":"Train"},{"location":"train/trainer/#class-trainer","text":"Class object to setup and carry the training. Takes as input a generator that produces SR images. Conditionally, also a discriminator network and a feature extractor to build the components of the perceptual loss. Compiles the model(s) and trains in a GANS fashion if a discriminator is provided, otherwise carries a regular ISR training.","title":"class Trainer"},{"location":"train/trainer/#args","text":"generator : Keras model, the super-scaling, or generator, network. discriminator : Keras model, the discriminator network for the adversarial component of the perceptual loss. feature_extractor : Keras model, feature extractor network for the deep features component of perceptual loss function. lr_train_dir : path to the directory containing the Low-Res images for training. hr_train_dir : path to the directory containing the High-Res images for training. lr_valid_dir : path to the directory containing the Low-Res images for validation. hr_valid_dir : path to the directory containing the High-Res images for validation. learning_rate : float. loss_weights : dictionary, use to weigh the components of the loss function. Contains 'generator' for the generator loss component, and can contain 'discriminator' and 'feature_extractor' for the discriminator and deep features components respectively. logs_dir : path to the directory where the tensorboard logs are saved. weights_dir : path to the directory where the weights are saved. dataname : string, used to identify what dataset is used for the training session. weights_generator : path to the pre-trained generator's weights, for transfer learning. weights_discriminator : path to the pre-trained discriminator's weights, for transfer learning. n_validation : integer, number of validation samples used at training from the validation set. flatness : dictionary. Determines determines the 'flatness' threshold level for the training patches. See the TrainerHelper class for more details. lr_decay_frequency : integer, every how many epochs the learning rate is reduced. lr_decay_factor : 0 < float <1, learning rate reduction multiplicative factor.","title":"Args"},{"location":"train/trainer/#methods","text":"train : combines the networks and triggers training with the specified settings.","title":"Methods"},{"location":"train/trainer/#9595init9595","text":"def __init__ ( generator , discriminator , feature_extractor , lr_train_dir , hr_train_dir , lr_valid_dir , hr_valid_dir , loss_weights , log_dirs , fallback_save_every_n_epochs , dataname , weights_generator , weights_discriminator , n_validation , flatness , learning_rate , adam_optimizer , losses , metrics )","title":"__init__"},{"location":"train/trainer/#update95training95config","text":"def update_training_config ( settings ) Summarizes training setting.","title":"update_training_config"},{"location":"train/trainer/#train","text":"def train ( epochs , steps_per_epoch , batch_size , monitored_metrics ) Carries on the training for the given number of epochs. Sends the losses to Tensorboard.","title":"train"},{"location":"train/trainer/#args_1","text":"epochs : how many epochs to train for. steps_per_epoch : how many batches epoch. batch_size : amount of images per batch. monitored_metrics : dictionary, the keys are the metrics that are monitored for the weights saving logic. The values are the mode that trigger the weights saving ('min' vs 'max').","title":"Args"},{"location":"tutorials/docker/","text":"Using ISR with Docker Setup Install Docker Clone our repository, get the sample weights from git lfs and cd into it: git clone https : // github . com / idealo / image - super - resolution git lfs pull cd image - super - resolution Build docker image for local usage docker build -t isr . -f Dockerfile.cpu In order to train remotely on AWS EC2 with GPU Install Docker Machine Install AWS Command Line Interface Set up an EC2 instance for training with GPU support. You can follow our nvidia-docker-keras project to get started Prediction Place your images ( png , jpg ) under data/input/<data name> , the results will be saved under /data/output/<data name>/<model>/<training setting> . NOTE: make sure that your images only have 3 layers (the png format allows for 4). Check the configuration file config.yml for more information on parameters and default folders. The -d flag in the run command will tell the program to load the weights specified in config.yml . It is possible though to iteratively select any option from the command line. Predict locally From the main folder run docker run - v $ ( pwd ) / data / : / home / isr / data - v $ ( pwd ) / weights / : / home / isr / weights - v $ ( pwd ) / isr / config . yml : / home / isr / config . yml - it isr - p - d - c config . yml Predict on AWS with nvidia-docker From the remote machine run (using our DockerHub image ) sudo nvidia - docker run - v $ ( pwd ) / isr / data / : / home / isr / data - v $ ( pwd ) / isr / weights / : / home / isr / weights - v $ ( pwd ) / isr / config . yml : / home / isr / config . yml - it idealo / image - super - resolution - gpu - p - d - c config . yml Training Train either locally with (or without) Docker, or on the cloud with nvidia-docker and AWS. Add you training set, including training and validation Low Res and High Res folders, under training_sets in config.yml . Train on AWS with GPU support using nvidia-docker To train with the default settings set in config.yml follow these steps: 1. From the main folder run bash scripts/setup.sh -m <name-of-ec2-instance> -b -i -u -d <data_name> . 2. ssh into the machine docker-machine ssh <name-of-ec2-instance> 3. Run training with sudo nvidia-docker run -v $(pwd)/isr/data/:/home/isr/data -v $(pwd)/isr/logs/:/home/isr/logs -v $(pwd)/isr/weights/:/home/isr/weights -v $(pwd)/isr/config.yml:/home/isr/config.yml -it isr -t -d -c config.yml <data_name> is the name of the folder containing your dataset. It must be under ./data/<data_name> . Tensorboard The log folder is mounted on the docker image. Open another EC2 terminal and run tensorboard --logdir /home/ubuntu/isr/logs and locally docker - machine ssh < name - of - ec2 - instance > - N - L 6006 : localhost : 6006 Notes A few helpful details - DO NOT include a Tensorflow version in requirements.txt as it would interfere with the version installed in the Tensorflow docker image - DO NOT use Ubuntu Server 18.04 LTS AMI. Use the Ubuntu Server 16.04 LTS AMI instead Train locally Train locally with docker From the main project folder run docker run - v $ ( pwd ) / data / : / home / isr / data - v $ ( pwd ) / logs / : / home / isr / logs - v $ ( pwd ) / weights / : / home / isr / weights - v $ ( pwd ) / isr / config . yml : / home / isr / config . yml - it isr - t - d - c config . yml","title":"Docker"},{"location":"tutorials/docker/#using-isr-with-docker","text":"","title":"Using ISR with Docker"},{"location":"tutorials/docker/#setup","text":"Install Docker Clone our repository, get the sample weights from git lfs and cd into it: git clone https : // github . com / idealo / image - super - resolution git lfs pull cd image - super - resolution Build docker image for local usage docker build -t isr . -f Dockerfile.cpu In order to train remotely on AWS EC2 with GPU Install Docker Machine Install AWS Command Line Interface Set up an EC2 instance for training with GPU support. You can follow our nvidia-docker-keras project to get started","title":"Setup"},{"location":"tutorials/docker/#prediction","text":"Place your images ( png , jpg ) under data/input/<data name> , the results will be saved under /data/output/<data name>/<model>/<training setting> . NOTE: make sure that your images only have 3 layers (the png format allows for 4). Check the configuration file config.yml for more information on parameters and default folders. The -d flag in the run command will tell the program to load the weights specified in config.yml . It is possible though to iteratively select any option from the command line.","title":"Prediction"},{"location":"tutorials/docker/#predict-locally","text":"From the main folder run docker run - v $ ( pwd ) / data / : / home / isr / data - v $ ( pwd ) / weights / : / home / isr / weights - v $ ( pwd ) / isr / config . yml : / home / isr / config . yml - it isr - p - d - c config . yml","title":"Predict locally"},{"location":"tutorials/docker/#predict-on-aws-with-nvidia-docker","text":"From the remote machine run (using our DockerHub image ) sudo nvidia - docker run - v $ ( pwd ) / isr / data / : / home / isr / data - v $ ( pwd ) / isr / weights / : / home / isr / weights - v $ ( pwd ) / isr / config . yml : / home / isr / config . yml - it idealo / image - super - resolution - gpu - p - d - c config . yml","title":"Predict on AWS with nvidia-docker"},{"location":"tutorials/docker/#training","text":"Train either locally with (or without) Docker, or on the cloud with nvidia-docker and AWS. Add you training set, including training and validation Low Res and High Res folders, under training_sets in config.yml .","title":"Training"},{"location":"tutorials/docker/#train-on-aws-with-gpu-support-using-nvidia-docker","text":"To train with the default settings set in config.yml follow these steps: 1. From the main folder run bash scripts/setup.sh -m <name-of-ec2-instance> -b -i -u -d <data_name> . 2. ssh into the machine docker-machine ssh <name-of-ec2-instance> 3. Run training with sudo nvidia-docker run -v $(pwd)/isr/data/:/home/isr/data -v $(pwd)/isr/logs/:/home/isr/logs -v $(pwd)/isr/weights/:/home/isr/weights -v $(pwd)/isr/config.yml:/home/isr/config.yml -it isr -t -d -c config.yml <data_name> is the name of the folder containing your dataset. It must be under ./data/<data_name> .","title":"Train on AWS with GPU support using nvidia-docker"},{"location":"tutorials/docker/#tensorboard","text":"The log folder is mounted on the docker image. Open another EC2 terminal and run tensorboard --logdir /home/ubuntu/isr/logs and locally docker - machine ssh < name - of - ec2 - instance > - N - L 6006 : localhost : 6006","title":"Tensorboard"},{"location":"tutorials/docker/#notes","text":"A few helpful details - DO NOT include a Tensorflow version in requirements.txt as it would interfere with the version installed in the Tensorflow docker image - DO NOT use Ubuntu Server 18.04 LTS AMI. Use the Ubuntu Server 16.04 LTS AMI instead","title":"Notes"},{"location":"tutorials/docker/#train-locally","text":"","title":"Train locally"},{"location":"tutorials/docker/#train-locally-with-docker","text":"From the main project folder run docker run - v $ ( pwd ) / data / : / home / isr / data - v $ ( pwd ) / logs / : / home / isr / logs - v $ ( pwd ) / weights / : / home / isr / weights - v $ ( pwd ) / isr / config . yml : / home / isr / config . yml - it isr - t - d - c config . yml","title":"Train locally with docker"},{"location":"tutorials/prediction/","text":"ISR Suite: HOW-TO Prediction Get the pre-trained weights and data Get the weights with wget https://github.com/idealo/image-super-resolution/raw/master/weights/sample_weights/rdn-C6-D20-G64-G064-x2/ArtefactCancelling/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5 wget https://github.com/idealo/image-super-resolution/raw/master/weights/sample_weights/rdn-C6-D20-G64-G064-x2/PSNR-driven/rdn-C6-D20-G64-G064-x2_PSNR_epoch086.hdf5 wget https://github.com/idealo/image-super-resolution/raw/master/weights/sample_weights/rdn-C3-D10-G64-G064-x2/PSNR-driven/rdn-C3-D10-G64-G064-x2_PSNR_epoch134.hdf5 mkdir weights mv *.hdf5 weights Download a sample image, in this case wget http://images.math.cnrs.fr/IMG/png/section8-image.png mkdir -p data/input/test_images mv *.png data/input/test_images Load the image with PIL, scale it and convert it into a format our model can use (it needs the extra dimension) import numpy as np from PIL import Image img = Image . open ( 'data/input/test_images/section8-image.png' ) lr_img = np . array ( img ) Get predictions Create the model and run prediction Create the RDN model, for which we provide pre-trained weights, and load them. Choose amongst the available model weights, compare the output if you wish. from ISR.models import RDN Large RDN model rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C6-D20-G64-G064-x2_PSNR_epoch086.hdf5' ) Small RDN model rdn = RDN ( arch_params = { 'C' : 3 , 'D' : 10 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C3-D10-G64-G064-x2_PSNR_epoch134.hdf5' ) Large RDN noise cancelling, detail enhancing model rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5' ) Run prediction sr_img = rdn . predict ( lr_img ) Image . fromarray ( sr_img ) Usecase: upscaling noisy images Now, for science, let's make it harder for the networks. We compress the image into the jpeg format to introduce compression artefact and lose some information. We will compare: the baseline bicubic scaling the basic model - Add Hyperlink a model trained to remove noise using perceptual loss with deep features and GANs training So let's first compress the image img . save ( 'data/input/test_images/compressed.jpeg' , 'JPEG' , dpi = [ 300 , 300 ], quality = 50 ) compressed_img = Image . open ( 'data/input/test_images/compressed.jpeg' ) compressed_lr_img = np . array ( compressed_img ) compressed_img . show () Baseline Bicubic scaling compressed_img . resize ( size = ( compressed_img . size [ 0 ] * 2 , compressed_img . size [ 1 ] * 2 ), resample = Image . BICUBIC ) Large RDN model (PSNR trained) rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C6-D20-G64-G064-x2_PSNR_epoch086.hdf5' ) sr_img = rdn . predict ( compressed_lr_img ) Image . fromarray ( sr_img ) Small RDN model (PSNR trained) rdn = RDN ( arch_params = { 'C' : 3 , 'D' : 10 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C3-D10-G64-G064-x2_PSNR_epoch134.hdf5' ) sr_img = rdn . predict ( compressed_lr_img ) Image . fromarray ( sr_img ) Large RDN noise cancelling, detail enhancing model rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5' ) sr_img = rdn . predict ( compressed_lr_img ) Image . fromarray ( sr_img ) Predictor Class You can also use the predictor class to run the model on entire folders. To do so you first need to create an output folder to collect your results, in this case data/output : from ISR.predict import Predictor predictor = Predictor ( input_dir = 'data/input/test_images/' , output_dir = 'data/output' ) predictor . get_predictions ( model = rdn , weights_path = 'weights/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5' )","title":"Prediction"},{"location":"tutorials/prediction/#isr-suite-how-to","text":"","title":"ISR Suite: HOW-TO"},{"location":"tutorials/prediction/#prediction","text":"","title":"Prediction"},{"location":"tutorials/prediction/#get-the-pre-trained-weights-and-data","text":"Get the weights with wget https://github.com/idealo/image-super-resolution/raw/master/weights/sample_weights/rdn-C6-D20-G64-G064-x2/ArtefactCancelling/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5 wget https://github.com/idealo/image-super-resolution/raw/master/weights/sample_weights/rdn-C6-D20-G64-G064-x2/PSNR-driven/rdn-C6-D20-G64-G064-x2_PSNR_epoch086.hdf5 wget https://github.com/idealo/image-super-resolution/raw/master/weights/sample_weights/rdn-C3-D10-G64-G064-x2/PSNR-driven/rdn-C3-D10-G64-G064-x2_PSNR_epoch134.hdf5 mkdir weights mv *.hdf5 weights Download a sample image, in this case wget http://images.math.cnrs.fr/IMG/png/section8-image.png mkdir -p data/input/test_images mv *.png data/input/test_images Load the image with PIL, scale it and convert it into a format our model can use (it needs the extra dimension) import numpy as np from PIL import Image img = Image . open ( 'data/input/test_images/section8-image.png' ) lr_img = np . array ( img )","title":"Get the pre-trained weights and data"},{"location":"tutorials/prediction/#get-predictions","text":"","title":"Get predictions"},{"location":"tutorials/prediction/#create-the-model-and-run-prediction","text":"Create the RDN model, for which we provide pre-trained weights, and load them. Choose amongst the available model weights, compare the output if you wish. from ISR.models import RDN","title":"Create the model and run prediction"},{"location":"tutorials/prediction/#large-rdn-model","text":"rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C6-D20-G64-G064-x2_PSNR_epoch086.hdf5' )","title":"Large RDN model"},{"location":"tutorials/prediction/#small-rdn-model","text":"rdn = RDN ( arch_params = { 'C' : 3 , 'D' : 10 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C3-D10-G64-G064-x2_PSNR_epoch134.hdf5' )","title":"Small RDN model"},{"location":"tutorials/prediction/#large-rdn-noise-cancelling-detail-enhancing-model","text":"rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5' )","title":"Large RDN noise cancelling, detail enhancing model"},{"location":"tutorials/prediction/#run-prediction","text":"sr_img = rdn . predict ( lr_img ) Image . fromarray ( sr_img )","title":"Run prediction"},{"location":"tutorials/prediction/#usecase-upscaling-noisy-images","text":"Now, for science, let's make it harder for the networks. We compress the image into the jpeg format to introduce compression artefact and lose some information. We will compare: the baseline bicubic scaling the basic model - Add Hyperlink a model trained to remove noise using perceptual loss with deep features and GANs training So let's first compress the image img . save ( 'data/input/test_images/compressed.jpeg' , 'JPEG' , dpi = [ 300 , 300 ], quality = 50 ) compressed_img = Image . open ( 'data/input/test_images/compressed.jpeg' ) compressed_lr_img = np . array ( compressed_img ) compressed_img . show ()","title":"Usecase: upscaling noisy images"},{"location":"tutorials/prediction/#baseline","text":"Bicubic scaling compressed_img . resize ( size = ( compressed_img . size [ 0 ] * 2 , compressed_img . size [ 1 ] * 2 ), resample = Image . BICUBIC )","title":"Baseline"},{"location":"tutorials/prediction/#large-rdn-model-psnr-trained","text":"rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C6-D20-G64-G064-x2_PSNR_epoch086.hdf5' ) sr_img = rdn . predict ( compressed_lr_img ) Image . fromarray ( sr_img )","title":"Large RDN model (PSNR trained)"},{"location":"tutorials/prediction/#small-rdn-model-psnr-trained","text":"rdn = RDN ( arch_params = { 'C' : 3 , 'D' : 10 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C3-D10-G64-G064-x2_PSNR_epoch134.hdf5' ) sr_img = rdn . predict ( compressed_lr_img ) Image . fromarray ( sr_img )","title":"Small RDN model  (PSNR trained)"},{"location":"tutorials/prediction/#large-rdn-noise-cancelling-detail-enhancing-model_1","text":"rdn = RDN ( arch_params = { 'C' : 6 , 'D' : 20 , 'G' : 64 , 'G0' : 64 , 'x' : 2 }) rdn . model . load_weights ( 'weights/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5' ) sr_img = rdn . predict ( compressed_lr_img ) Image . fromarray ( sr_img )","title":"Large RDN noise cancelling, detail enhancing model"},{"location":"tutorials/prediction/#predictor-class","text":"You can also use the predictor class to run the model on entire folders. To do so you first need to create an output folder to collect your results, in this case data/output : from ISR.predict import Predictor predictor = Predictor ( input_dir = 'data/input/test_images/' , output_dir = 'data/output' ) predictor . get_predictions ( model = rdn , weights_path = 'weights/rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5' )","title":"Predictor Class"},{"location":"tutorials/training/","text":"ISR Suite: HOW-TO Training Get the training data Get your data to train the model. The div2k dataset linked here is for a scaling factor of 2. Beware of this later when training the model. wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X2.zip wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X2.zip wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip mkdir div2k unzip -q DIV2K_valid_LR_bicubic_X2.zip -d div2k unzip -q DIV2K_train_LR_bicubic_X2.zip -d div2k unzip -q DIV2K_train_HR.zip -d div2k unzip -q DIV2K_valid_HR.zip -d div2k Create the models Import the models from the ISR package and create a RRDN super scaling network a discriminator network for GANs training a VGG19 feature extractor to train with a perceptual loss function Carefully select: 'x': this is the upscaling factor (2 by default) 'layers_to_extract': these are the layers from the VGG19 that will be used in the perceptual loss (leave the default if you're not familiar with it) 'lr_patch_size': this is the size of the patches that will be extracted from the LR images and fed to the ISR network during training time Play around with the other architecture parameters from ISR.models import RRDN from ISR.models import Discriminator from ISR.models import Cut_VGG19 lr_train_patch_size = 40 layers_to_extract = [ 5 , 9 ] scale = 2 hr_train_patch_size = lr_train_patch_size * scale rrdn = RRDN ( arch_params = { 'C' : 4 , 'D' : 3 , 'G' : 64 , 'G0' : 64 , 'T' : 10 , 'x' : scale }, patch_size = lr_train_patch_size ) f_ext = Cut_VGG19 ( patch_size = hr_train_patch_size , layers_to_extract = layers_to_extract ) discr = Discriminator ( patch_size = hr_train_patch_size , kernel_size = 3 ) Give the models to the Trainer The Trainer object will combine the networks, manage your training data and keep you up-to-date with the training progress through Tensorboard and the command line. from ISR.train import Trainer loss_weights = { 'generator' : 0.0 , 'feature_extractor' : 0.0833 , 'discriminator' : 0.01 } losses = { 'generator' : 'mae' , 'feature_extractor' : 'mse' , 'discriminator' : 'binary_crossentropy' } log_dirs = { 'logs' : './logs' , 'weights' : './weights' } learning_rate = { 'initial_value' : 0.0004 , 'decay_factor' : 0.5 , 'decay_frequency' : 30 } flatness = { 'min' : 0.0 , 'max' : 0.15 , 'increase' : 0.01 , 'increase_frequency' : 5 } adam_optimizer = { 'beta1' : 0.9 , 'beta2' : 0.999 , 'epsilon' : None } trainer = Trainer ( generator = rrdn , discriminator = discr , feature_extractor = f_ext , lr_train_dir = 'div2k/DIV2K_train_LR_bicubic/X2/' , hr_train_dir = 'div2k/DIV2K_train_HR/' , lr_valid_dir = 'div2k/DIV2K_train_LR_bicubic/X2/' , hr_valid_dir = 'div2k/DIV2K_train_HR/' , loss_weights = loss_weights , losses = losses , learning_rate = learning_rate , flatness = flatness , log_dirs = log_dirs , adam_optimizer = adam_optimizer , metrics = { 'generator' : 'PSNR_Y' }, dataname = 'div2k' , weights_generator = None , weights_discriminator = None , n_validation = 40 , ) Choose epoch number, steps and batch size and start training trainer . train ( epochs = 1 , steps_per_epoch = 20 , batch_size = 4 , monitored_metrics = { 'val_generator_loss' : 'min' } )","title":"Training"},{"location":"tutorials/training/#isr-suite-how-to","text":"","title":"ISR Suite: HOW-TO"},{"location":"tutorials/training/#training","text":"","title":"Training"},{"location":"tutorials/training/#get-the-training-data","text":"Get your data to train the model. The div2k dataset linked here is for a scaling factor of 2. Beware of this later when training the model. wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X2.zip wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X2.zip wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip mkdir div2k unzip -q DIV2K_valid_LR_bicubic_X2.zip -d div2k unzip -q DIV2K_train_LR_bicubic_X2.zip -d div2k unzip -q DIV2K_train_HR.zip -d div2k unzip -q DIV2K_valid_HR.zip -d div2k","title":"Get the training data"},{"location":"tutorials/training/#create-the-models","text":"Import the models from the ISR package and create a RRDN super scaling network a discriminator network for GANs training a VGG19 feature extractor to train with a perceptual loss function Carefully select: 'x': this is the upscaling factor (2 by default) 'layers_to_extract': these are the layers from the VGG19 that will be used in the perceptual loss (leave the default if you're not familiar with it) 'lr_patch_size': this is the size of the patches that will be extracted from the LR images and fed to the ISR network during training time Play around with the other architecture parameters from ISR.models import RRDN from ISR.models import Discriminator from ISR.models import Cut_VGG19 lr_train_patch_size = 40 layers_to_extract = [ 5 , 9 ] scale = 2 hr_train_patch_size = lr_train_patch_size * scale rrdn = RRDN ( arch_params = { 'C' : 4 , 'D' : 3 , 'G' : 64 , 'G0' : 64 , 'T' : 10 , 'x' : scale }, patch_size = lr_train_patch_size ) f_ext = Cut_VGG19 ( patch_size = hr_train_patch_size , layers_to_extract = layers_to_extract ) discr = Discriminator ( patch_size = hr_train_patch_size , kernel_size = 3 )","title":"Create the models"},{"location":"tutorials/training/#give-the-models-to-the-trainer","text":"The Trainer object will combine the networks, manage your training data and keep you up-to-date with the training progress through Tensorboard and the command line. from ISR.train import Trainer loss_weights = { 'generator' : 0.0 , 'feature_extractor' : 0.0833 , 'discriminator' : 0.01 } losses = { 'generator' : 'mae' , 'feature_extractor' : 'mse' , 'discriminator' : 'binary_crossentropy' } log_dirs = { 'logs' : './logs' , 'weights' : './weights' } learning_rate = { 'initial_value' : 0.0004 , 'decay_factor' : 0.5 , 'decay_frequency' : 30 } flatness = { 'min' : 0.0 , 'max' : 0.15 , 'increase' : 0.01 , 'increase_frequency' : 5 } adam_optimizer = { 'beta1' : 0.9 , 'beta2' : 0.999 , 'epsilon' : None } trainer = Trainer ( generator = rrdn , discriminator = discr , feature_extractor = f_ext , lr_train_dir = 'div2k/DIV2K_train_LR_bicubic/X2/' , hr_train_dir = 'div2k/DIV2K_train_HR/' , lr_valid_dir = 'div2k/DIV2K_train_LR_bicubic/X2/' , hr_valid_dir = 'div2k/DIV2K_train_HR/' , loss_weights = loss_weights , losses = losses , learning_rate = learning_rate , flatness = flatness , log_dirs = log_dirs , adam_optimizer = adam_optimizer , metrics = { 'generator' : 'PSNR_Y' }, dataname = 'div2k' , weights_generator = None , weights_discriminator = None , n_validation = 40 , ) Choose epoch number, steps and batch size and start training trainer . train ( epochs = 1 , steps_per_epoch = 20 , batch_size = 4 , monitored_metrics = { 'val_generator_loss' : 'min' } )","title":"Give the models to the Trainer"},{"location":"utils/datahandler/","text":"class DataHandler DataHandler generate augmented batches used for training or validation. Args lr_dir : directory containing the Low Res images. hr_dir : directory containing the High Res images. patch_size : integer, size of the patches extracted from LR images. scale : integer, upscaling factor. n_validation_samples : integer, size of the validation set. Only provided if the DataHandler is used to generate validation sets. __init__ def __init__ ( lr_dir , hr_dir , patch_size , scale , n_validation_samples ) get_batch def get_batch ( batch_size , idx , flatness ) Returns a dictionary with keys ('lr', 'hr') containing training batches of Low Res and High Res image patches. Args batch_size : integer. flatness : float in [0,1], is the patch \"flatness\" threshold. Determines what level of detail the patches need to meet. 0 means any patch is accepted. get_validation_batches def get_validation_batches ( batch_size ) Returns a batch for each image in the validation set. get_validation_set def get_validation_set ( batch_size ) Returns a batch for each image in the validation set. Flattens and splits them to feed it to Keras's model.evaluate.","title":"Data Handler"},{"location":"utils/datahandler/#class-datahandler","text":"DataHandler generate augmented batches used for training or validation.","title":"class DataHandler"},{"location":"utils/datahandler/#args","text":"lr_dir : directory containing the Low Res images. hr_dir : directory containing the High Res images. patch_size : integer, size of the patches extracted from LR images. scale : integer, upscaling factor. n_validation_samples : integer, size of the validation set. Only provided if the DataHandler is used to generate validation sets.","title":"Args"},{"location":"utils/datahandler/#9595init9595","text":"def __init__ ( lr_dir , hr_dir , patch_size , scale , n_validation_samples )","title":"__init__"},{"location":"utils/datahandler/#get95batch","text":"def get_batch ( batch_size , idx , flatness ) Returns a dictionary with keys ('lr', 'hr') containing training batches of Low Res and High Res image patches.","title":"get_batch"},{"location":"utils/datahandler/#args_1","text":"batch_size : integer. flatness : float in [0,1], is the patch \"flatness\" threshold. Determines what level of detail the patches need to meet. 0 means any patch is accepted.","title":"Args"},{"location":"utils/datahandler/#get95validation95batches","text":"def get_validation_batches ( batch_size ) Returns a batch for each image in the validation set.","title":"get_validation_batches"},{"location":"utils/datahandler/#get95validation95set","text":"def get_validation_set ( batch_size ) Returns a batch for each image in the validation set. Flattens and splits them to feed it to Keras's model.evaluate.","title":"get_validation_set"},{"location":"utils/image_processing/","text":"process_array def process_array ( image_array ) Process a 3-dimensional array into a scaled, 4 dimensional batch of size 1. process_output def process_output ( output_tensor ) Transforms the 4-dimensional output tensor into a suitable image format.","title":"Image processing"},{"location":"utils/image_processing/#process95array","text":"def process_array ( image_array ) Process a 3-dimensional array into a scaled, 4 dimensional batch of size 1.","title":"process_array"},{"location":"utils/image_processing/#process95output","text":"def process_output ( output_tensor ) Transforms the 4-dimensional output tensor into a suitable image format.","title":"process_output"},{"location":"utils/logger/","text":"get_logger def get_logger ( name , job_dir ) Returns logger that prints on stdout at INFO level and on file at DEBUG level.","title":"Logger"},{"location":"utils/logger/#get95logger","text":"def get_logger ( name , job_dir ) Returns logger that prints on stdout at INFO level and on file at DEBUG level.","title":"get_logger"},{"location":"utils/metrics/","text":"PSNR def PSNR ( y_true , y_pred , MAXp ) Evaluates the PSNR value: PSNR = 20 * log10(MAXp) - 10 * log10(MSE). Args y_true : ground truth. y_pred : predicted value. MAXp : maximum value of the pixel range (default=1). RGB_to_Y def RGB_to_Y ( image ) Image has values from 0 to 1. PSNR_Y def PSNR_Y ( y_true , y_pred , MAXp ) Evaluates the PSNR value on the Y channel: PSNR = 20 * log10(MAXp) - 10 * log10(MSE). Args y_true : ground truth. y_pred : predicted value. MAXp : maximum value of the pixel range (default=1).","title":"Metrics"},{"location":"utils/metrics/#psnr","text":"def PSNR ( y_true , y_pred , MAXp ) Evaluates the PSNR value: PSNR = 20 * log10(MAXp) - 10 * log10(MSE).","title":"PSNR"},{"location":"utils/metrics/#args","text":"y_true : ground truth. y_pred : predicted value. MAXp : maximum value of the pixel range (default=1).","title":"Args"},{"location":"utils/metrics/#rgb95to95y","text":"def RGB_to_Y ( image ) Image has values from 0 to 1.","title":"RGB_to_Y"},{"location":"utils/metrics/#psnr95y","text":"def PSNR_Y ( y_true , y_pred , MAXp ) Evaluates the PSNR value on the Y channel: PSNR = 20 * log10(MAXp) - 10 * log10(MSE).","title":"PSNR_Y"},{"location":"utils/metrics/#args_1","text":"y_true : ground truth. y_pred : predicted value. MAXp : maximum value of the pixel range (default=1).","title":"Args"},{"location":"utils/train_helper/","text":"class TrainerHelper Collection of useful functions to manage training sessions. Args generator : Keras model, the super-scaling, or generator, network. logs_dir : path to the directory where the tensorboard logs are saved. weights_dir : path to the directory where the weights are saved. lr_train_dir : path to the directory containing the Low-Res images. feature_extractor : Keras model, feature extractor network for the deep features component of perceptual loss function. discriminator : Keras model, the discriminator network for the adversarial component of the perceptual loss. dataname : string, used to identify what dataset is used for the training session. fallback_save_every_n_epochs : integer, determines after how many epochs that did not trigger weights saving the weights are despite no metric improvement. max_n_best_weights : maximum amount of weights that are best on some metric that are kept. max_n_other_weights : maximum amount of non-best weights that are kept. Methods print_training_setting : see docstring. on_epoch_end : see docstring. epoch_n_from_weights_name : see docstring. initialize_training : see docstring. __init__ def __init__ ( generator , weights_dir , logs_dir , lr_train_dir , feature_extractor , discriminator , dataname , weights_generator , weights_discriminator , fallback_save_every_n_epochs , max_n_other_weights , max_n_best_weights ) get_session_id def get_session_id ( basename ) Returns unique session identifier. update_config def update_config ( training_settings ) Adds to the existing settings (if any) the current settings dictionary under the session_id key. print_training_setting def print_training_setting ( settings ) Does what it says. on_epoch_end def on_epoch_end ( epoch , losses , generator , discriminator , metrics ) Manages the operations that are taken at the end of each epoch: metric checks, weight saves, logging. epoch_n_from_weights_name def epoch_n_from_weights_name ( w_name ) Extracts the last epoch number from the standardized weights name. Only works if the weights contain 'epoch' followed by 3 integers, for example: some-architectureepoch023suffix.hdf5 initialize_training def initialize_training ( object ) Function that is exectured prior to training. Wraps up most of the functions of this class: load the weights if any are given, generaters names for session and weights, creates directories and prints the training session.","title":"Train Helper"},{"location":"utils/train_helper/#class-trainerhelper","text":"Collection of useful functions to manage training sessions.","title":"class TrainerHelper"},{"location":"utils/train_helper/#args","text":"generator : Keras model, the super-scaling, or generator, network. logs_dir : path to the directory where the tensorboard logs are saved. weights_dir : path to the directory where the weights are saved. lr_train_dir : path to the directory containing the Low-Res images. feature_extractor : Keras model, feature extractor network for the deep features component of perceptual loss function. discriminator : Keras model, the discriminator network for the adversarial component of the perceptual loss. dataname : string, used to identify what dataset is used for the training session. fallback_save_every_n_epochs : integer, determines after how many epochs that did not trigger weights saving the weights are despite no metric improvement. max_n_best_weights : maximum amount of weights that are best on some metric that are kept. max_n_other_weights : maximum amount of non-best weights that are kept.","title":"Args"},{"location":"utils/train_helper/#methods","text":"print_training_setting : see docstring. on_epoch_end : see docstring. epoch_n_from_weights_name : see docstring. initialize_training : see docstring.","title":"Methods"},{"location":"utils/train_helper/#9595init9595","text":"def __init__ ( generator , weights_dir , logs_dir , lr_train_dir , feature_extractor , discriminator , dataname , weights_generator , weights_discriminator , fallback_save_every_n_epochs , max_n_other_weights , max_n_best_weights )","title":"__init__"},{"location":"utils/train_helper/#get95session95id","text":"def get_session_id ( basename ) Returns unique session identifier.","title":"get_session_id"},{"location":"utils/train_helper/#update95config","text":"def update_config ( training_settings ) Adds to the existing settings (if any) the current settings dictionary under the session_id key.","title":"update_config"},{"location":"utils/train_helper/#print95training95setting","text":"def print_training_setting ( settings ) Does what it says.","title":"print_training_setting"},{"location":"utils/train_helper/#on95epoch95end","text":"def on_epoch_end ( epoch , losses , generator , discriminator , metrics ) Manages the operations that are taken at the end of each epoch: metric checks, weight saves, logging.","title":"on_epoch_end"},{"location":"utils/train_helper/#epoch95n95from95weights95name","text":"def epoch_n_from_weights_name ( w_name ) Extracts the last epoch number from the standardized weights name. Only works if the weights contain 'epoch' followed by 3 integers, for example: some-architectureepoch023suffix.hdf5","title":"epoch_n_from_weights_name"},{"location":"utils/train_helper/#initialize95training","text":"def initialize_training ( object ) Function that is exectured prior to training. Wraps up most of the functions of this class: load the weights if any are given, generaters names for session and weights, creates directories and prints the training session.","title":"initialize_training"},{"location":"utils/utils/","text":"parse_args def parse_args () Parse CLI arguments. get_timestamp def get_timestamp () check_parameter_keys def check_parameter_keys ( parameter , needed_keys , optional_keys , default_value ) get_config_from_weights def get_config_from_weights ( w_path , arch_params , name ) Extracts architecture parameters from the file name of the weights. Only works with standardized weights name. select_option def select_option ( options , message , val ) CLI selection given options. select_multiple_options def select_multiple_options ( options , message , val ) CLI multiple selection given options. select_bool def select_bool ( message ) CLI bool selection. select_positive_float def select_positive_float ( message ) CLI non-negative float selection. select_positive_integer def select_positive_integer ( message , value ) CLI non-negative integer selection. browse_weights def browse_weights ( weights_dir , model ) Weights selection from cl. setup def setup ( config_file , default , training , prediction ) CLI interface to set up the training or prediction session. Takes as input the configuration file path (minus the '.py' extension) and arguments parse from CLI. suggest_metrics def suggest_metrics ( discriminator , feature_extractor , loss_weights ) select_dataset def select_dataset ( session_type , conf ) CLI snippet for selection the dataset for training.","title":"Utils"},{"location":"utils/utils/#parse95args","text":"def parse_args () Parse CLI arguments.","title":"parse_args"},{"location":"utils/utils/#get95timestamp","text":"def get_timestamp ()","title":"get_timestamp"},{"location":"utils/utils/#check95parameter95keys","text":"def check_parameter_keys ( parameter , needed_keys , optional_keys , default_value )","title":"check_parameter_keys"},{"location":"utils/utils/#get95config95from95weights","text":"def get_config_from_weights ( w_path , arch_params , name ) Extracts architecture parameters from the file name of the weights. Only works with standardized weights name.","title":"get_config_from_weights"},{"location":"utils/utils/#select95option","text":"def select_option ( options , message , val ) CLI selection given options.","title":"select_option"},{"location":"utils/utils/#select95multiple95options","text":"def select_multiple_options ( options , message , val ) CLI multiple selection given options.","title":"select_multiple_options"},{"location":"utils/utils/#select95bool","text":"def select_bool ( message ) CLI bool selection.","title":"select_bool"},{"location":"utils/utils/#select95positive95float","text":"def select_positive_float ( message ) CLI non-negative float selection.","title":"select_positive_float"},{"location":"utils/utils/#select95positive95integer","text":"def select_positive_integer ( message , value ) CLI non-negative integer selection.","title":"select_positive_integer"},{"location":"utils/utils/#browse95weights","text":"def browse_weights ( weights_dir , model ) Weights selection from cl.","title":"browse_weights"},{"location":"utils/utils/#setup","text":"def setup ( config_file , default , training , prediction ) CLI interface to set up the training or prediction session. Takes as input the configuration file path (minus the '.py' extension) and arguments parse from CLI.","title":"setup"},{"location":"utils/utils/#suggest95metrics","text":"def suggest_metrics ( discriminator , feature_extractor , loss_weights )","title":"suggest_metrics"},{"location":"utils/utils/#select95dataset","text":"def select_dataset ( session_type , conf ) CLI snippet for selection the dataset for training.","title":"select_dataset"}]}